{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5197d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from skimage.feature import hog\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import PCA\n",
    "from datetime import datetime\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gc\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122993c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 128\n",
    "DATASET_PATH = \"dataset\"\n",
    "CLASSES = [\"correct_mask\", \"incorrect_mask\"]\n",
    "LOG_FILE = \"results_log.txt\"\n",
    "MODEL_FILE = \"hog_svm_model_optimized.pkl\"\n",
    "BATCH_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f5c82a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# İyileştirilmiş HOG parametreleri - daha zengin feature extraction\n",
    "HOG_PARAMS = {\n",
    "    'orientations': 12,  # 9'dan 12'ye artırıldı - daha fazla gradient yönü\n",
    "    'pixels_per_cell': (6, 6),  # (8,8)'den (6,6)'ya küçültüldü - daha detaylı\n",
    "    'cells_per_block': (3, 3),  # (2,2)'den (3,3)'e artırıldı - daha iyi normalizasyon\n",
    "    'block_norm': 'L2-Hys',\n",
    "    'transform_sqrt': True,\n",
    "    'feature_vector': True\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea67aeb",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Satırdakiler eski parametreler\n",
    "SVM_PARAMS = {\n",
    "    'C': 10.0,  # 1.0 \n",
    "    'max_iter': 5000,  # 2000\n",
    "    'random_state': 42,\n",
    "    'dual': False,\n",
    "    'tol': 1e-5,  # 1e-4 \n",
    "    'class_weight': 'balanced'  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2af3902",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def extract_hog_features(image, hog_params=HOG_PARAMS):\n",
    "    features = hog(image, **hog_params)\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbff4b0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def preprocess_image(image, img_size=IMG_SIZE):\n",
    "    # CLAHE (Contrast Limited Adaptive Histogram Equalization) ekle\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8,8))\n",
    "    image = clahe.apply(image)\n",
    "    \n",
    "    # edge preserving\n",
    "    image = cv2.bilateralFilter(image, 5, 50, 50)\n",
    "    \n",
    "    image = cv2.resize(image, (img_size, img_size), interpolation=cv2.INTER_LANCZOS4)  # Daha kaliteli resize\n",
    "    image = image.astype(np.float32) / 255.0\n",
    "    \n",
    "    image = (image - image.min()) / (image.max() - image.min())\n",
    "    image = (image * 255).astype(np.uint8)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc28810",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_data_efficiently(): # olmazsa çok uzun sürüyor\n",
    "    all_features = []\n",
    "    all_labels = []\n",
    "\n",
    "    for label, class_name in enumerate(CLASSES):\n",
    "        folder_path = os.path.join(DATASET_PATH, class_name)\n",
    "        files_list = [os.path.join(root, file) for root, _, files in os.walk(folder_path) for file in files if file.lower().endswith((\".jpg\", \".png\", \".jpeg\", \".bmp\"))]\n",
    "        \n",
    "        batch_features = []\n",
    "        batch_labels = []\n",
    "        \n",
    "        for img_path in tqdm(files_list, desc=f\"Processing {class_name}\"):\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)\n",
    "            img_processed = preprocess_image(img)\n",
    "            hog_feat = extract_hog_features(img_processed)\n",
    "            \n",
    "            batch_features.append(hog_feat)\n",
    "            batch_labels.append(label)\n",
    "            \n",
    "            # RAM için\n",
    "            if len(batch_features) >= 500:\n",
    "                all_features.extend(batch_features)\n",
    "                all_labels.extend(batch_labels)\n",
    "                batch_features = []\n",
    "                batch_labels = []\n",
    "                gc.collect()\n",
    "        \n",
    "        all_features.extend(batch_features)\n",
    "        all_labels.extend(batch_labels)\n",
    "        gc.collect()\n",
    "    \n",
    "    return np.array(all_features, dtype=np.float32), np.array(all_labels)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd98e616",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def optimize_features(X, n_components=0.95):  # Memory için düşürüldü\n",
    "    print(\"Memory-efficient PCA uygulanıyor...\")\n",
    "    \n",
    "    # memory efficient\n",
    "    from sklearn.decomposition import IncrementalPCA\n",
    "    \n",
    "    batch_size = min(1000, X.shape[0] // 10)\n",
    "    \n",
    "    if isinstance(n_components, float):\n",
    "        sample_size = min(2000, X.shape[0])\n",
    "        sample_indices = np.random.choice(X.shape[0], sample_size, replace=False)\n",
    "        X_sample = X[sample_indices]\n",
    "        \n",
    "        temp_pca = PCA(random_state=42)\n",
    "        temp_pca.fit(X_sample)\n",
    "        \n",
    "        cumsum = np.cumsum(temp_pca.explained_variance_ratio_)\n",
    "        n_components = np.argmax(cumsum >= n_components) + 1\n",
    "        print(f\"PCA component sayısı: {n_components}\")\n",
    "        \n",
    "        del temp_pca, X_sample\n",
    "        gc.collect()\n",
    "    \n",
    "    pca = IncrementalPCA(n_components=n_components, batch_size=batch_size)\n",
    "    \n",
    "    for i in range(0, X.shape[0], batch_size):\n",
    "        batch = X[i:i+batch_size]\n",
    "        pca.partial_fit(batch)\n",
    "    \n",
    "    # Transform\n",
    "    X_reduced = []\n",
    "    for i in range(0, X.shape[0], batch_size):\n",
    "        batch = X[i:i+batch_size]\n",
    "        X_reduced.append(pca.transform(batch))\n",
    "    \n",
    "    X_reduced = np.vstack(X_reduced)\n",
    "    \n",
    "    print(f\"PCA: {X.shape[1]} -> {X_reduced.shape[1]} features\")\n",
    "    print(f\"Açıklanan varyans: {pca.explained_variance_ratio_.sum():.4f}\")\n",
    "    \n",
    "    return X_reduced, pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a021ba",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def hyperparameter_optimization(X_train, y_train):\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('svm', LinearSVC(random_state=42, max_iter=5000))\n",
    "    ])\n",
    "\n",
    "    # Memory için daha küçük grid\n",
    "    param_grid = {\n",
    "        'svm__C': [1.0, 5.0, 10.0, 20.0],  # Daha az seçenek\n",
    "        'svm__loss': ['hinge', 'squared_hinge'],\n",
    "        'svm__dual': [False],\n",
    "        'svm__tol': [1e-4, 1e-3],  # Daha az seçenek\n",
    "        'svm__class_weight': ['balanced'],  # Sadece balanced\n",
    "        'svm__max_iter': [5000]  # Sabit değer\n",
    "    }\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        pipeline,\n",
    "        param_grid,\n",
    "        cv=3,  # 5\n",
    "        scoring='accuracy',\n",
    "        n_jobs=2,  # 2\n",
    "        verbose=1,\n",
    "        return_train_score=True\n",
    "    )\n",
    "\n",
    "    print(\"Hiperparametre optimizasyonu başlıyor...\")\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(f\"En iyi parametreler: {grid_search.best_params_}\")\n",
    "    print(f\"En iyi CV skoru: {grid_search.best_score_:.4f}\")\n",
    "    \n",
    "    return grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d44dc82",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def train_and_evaluate():\n",
    "    print(\"Veri yükleniyor...\")\n",
    "    X, y = load_data_efficiently()\n",
    "    print(f\"Toplam veri: {X.shape[0]} örnek, {X.shape[1]} feature\")\n",
    "    \n",
    "    # Memory temizliği\n",
    "    gc.collect()\n",
    "    \n",
    "    if X.shape[1] > 2000:  # Threshold düşürüldü\n",
    "        print(\"Memory-efficient PCA uygulanıyor...\")\n",
    "        X, pca = optimize_features(X, n_components=0.95)  # 0.98\n",
    "        joblib.dump(pca, \"pca_model.pkl\")\n",
    "        gc.collect()\n",
    "    else:\n",
    "        pca = None\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "    \n",
    "    print(f\"Eğitim seti: {X_train.shape[0]} örnek\")\n",
    "    print(f\"Test seti: {X_test.shape[0]} örnek\")\n",
    "    \n",
    "    # Memory temizliği\n",
    "    del X, y\n",
    "    gc.collect()\n",
    "\n",
    "    # Basit model ile karşılaştırma\n",
    "    print(\"\\nBasit model eğitiliyor...\")\n",
    "    simple_model = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('svm', LinearSVC(**SVM_PARAMS))\n",
    "    ])\n",
    "    simple_model.fit(X_train, y_train)\n",
    "    simple_pred = simple_model.predict(X_test)\n",
    "    simple_accuracy = accuracy_score(y_test, simple_pred)\n",
    "    print(f\"Basit model doğruluğu: {simple_accuracy:.4f}\")\n",
    "\n",
    "    # Memory temizliği\n",
    "    del simple_model, simple_pred\n",
    "    gc.collect()\n",
    "\n",
    "    # Optimized model\n",
    "    print(\"\\nOptimize edilmiş model eğitiliyor...\")\n",
    "    optimized_model = hyperparameter_optimization(X_train, y_train)\n",
    "    optimized_pred = optimized_model.predict(X_test)\n",
    "    optimized_accuracy = accuracy_score(y_test, optimized_pred)\n",
    "    \n",
    "    print(f\"Optimize edilmiş model doğruluğu: {optimized_accuracy:.4f}\")\n",
    "    print(f\"İyileştirme: {optimized_accuracy - simple_accuracy:.4f}\")\n",
    "\n",
    "    # Cross-validation skorları (memory için küçültüldü)\n",
    "    cv_scores = cross_val_score(optimized_model, X_train, y_train, cv=3)\n",
    "    print(f\"CV skorları: {cv_scores}\")\n",
    "    print(f\"Ortalama CV skoru: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\")\n",
    "\n",
    "    print(\"\\nDetaylı classification report:\")\n",
    "    print(classification_report(y_test, optimized_pred, target_names=CLASSES))\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_test, optimized_pred)\n",
    "    print(f\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "\n",
    "    # Model kaydet\n",
    "    joblib.dump(optimized_model, MODEL_FILE)\n",
    "    print(f\"Model kaydedildi: {MODEL_FILE}\")\n",
    "    \n",
    "    # Log dosyasına yaz\n",
    "    with open(LOG_FILE, 'a') as f:\n",
    "        f.write(f\"\\n{datetime.now()}\\n\")\n",
    "        f.write(f\"Basit model doğruluğu: {simple_accuracy:.4f}\\n\")\n",
    "        f.write(f\"Optimize edilmiş model doğruluğu: {optimized_accuracy:.4f}\\n\")\n",
    "        f.write(f\"İyileştirme: {optimized_accuracy - simple_accuracy:.4f}\\n\")\n",
    "        f.write(f\"CV skoru: {cv_scores.mean():.4f} (+/- {cv_scores.std() * 2:.4f})\\n\")\n",
    "        f.write(\"-\" * 50 + \"\\n\")\n",
    "    \n",
    "    return optimized_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3da033fe",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def predict_single_image(model_path, image_path, pca_path=None):\n",
    "    model = joblib.load(model_path)\n",
    "    pca = joblib.load(pca_path) if pca_path and os.path.exists(pca_path) else None\n",
    "\n",
    "    img = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    img_processed = preprocess_image(img)\n",
    "    hog_feat = extract_hog_features(img_processed)\n",
    "    features = hog_feat.reshape(1, -1)\n",
    "    \n",
    "    if pca:\n",
    "        features = pca.transform(features)\n",
    "\n",
    "    prediction = model.predict(features)[0]\n",
    "    confidence = model.decision_function(features)[0]\n",
    "    result = CLASSES[prediction]\n",
    "    \n",
    "    print(f\"Tahmin: {result}\")\n",
    "    print(f\"Güven skoru: {confidence:.4f}\")\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bc86287",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"HOG+SVM Mask Classification - İyileştirilmiş Versiyon\")\n",
    "    print(\"=\" * 60)\n",
    "    model = train_and_evaluate()\n",
    "    print(\"\\nEğitim tamamlandı!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19399b93",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
